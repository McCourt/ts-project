---
title: "Final Project"
author: "McCourt Hu, Lin Zuo, Jingyi Zhang, Yuanling Wang"
date: "12/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(forecast)
library(grid)
library(gridExtra)
set.seed(20181208)
df = read.csv("data.csv")
df$Date = as.Date(df$Date, format = "%Y/%m/%d")
df$Open = as.numeric(as.character(df$Open))
df$fb_close = as.numeric(as.character(df$fb_close))
df$google_close = as.numeric(as.character(df$google_close))
df$apple_close = as.numeric(as.character(df$apple_close))
rmse = function(pred, truth){
  sqrt(mean((pred - truth)^2))
}
```

## Introduction

For this particular project, we are looking at the stock prices of four major tech companies: Apple, Facebook, Amazon and Google, from the years of, correspondingly, 1980, 2012, 1997 and 2004 to the year of 2018. With the data downloaded from Kaggle (*https://www.kaggle.com/stexo92/gafa-stock-prices*), we got access to the dates, opening prices, closing prices, stock volume, highest/lowest prices and adjusted close prices of these companies.

Based on the nature of stock market, there can potentially be temporal structures when analyzing and predicting stock prices. We take the difference between the opening and the closing prices of the stocks as the response variable and try to fit appropriate models to help determine what affects the change in stock prices each day for each company, as well as understanding the temporal structures within.

We will first look at some Explanatory Data Analyses for all four companies, then try to fit simpler models with no temporal structures, and finally fit and evaluate temporal models for each company. For the temporal model fitting part specifically, we will try two different methods: both auto-fitting ARIMA models, as well as models with Gaussian Process. We would then compare the performances of all three types models fit by both methods.

Lastly, we wish to come to a conclusion for our questions of interest: what are the factors that can potentially affect the closing prices of stocks? Is there any temporal dependency in the closing prices? Are there differences among different companies or they share similar trends and structures in their stocks?  We would also have a discussion on the adequacy, potential problems with the models and provide suggestions for developing this project.

## Exploratory Data Analysis

```{r message=FALSE}
summary(df)
p1 = ggplot(data = df %>% arrange(Date), aes(x = Date, y = fb_close)) +
  geom_line() +
  ggtitle("Stock Close Price of Facebook")
p2 = ggplot(data = df %>% arrange(Date), aes(x = Date, y = apple_close)) +
  geom_line() +
  ggtitle("Stock Close Price of Apple")
p3 = ggplot(data = df %>% arrange(Date), aes(x = Date, y = google_close)) +
  geom_line() +
  ggtitle("Stock Close Price of Google")
p4 = ggplot(data = df %>% arrange(Date), aes(x = Date, y = Open)) +
  geom_line() +
  ggtitle("S&P 500 Index Open")
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

Among all variables available in the dataset, we only focused on two variables `Date` and `Close`. Reasons why we didn't use other variables are as follows:
1. `Open`: The correlation between variables `Open` and `Close` is almost one. This result intuitively makes sense because the average magnitude of daily stock price fluctuation is minimal compared with stock price itself. However, since `Open` and `Close` are almost perfectly correlated, we thought that using `Open` as the predictor for `Close` will not help us understand the research question that we're interested. Therefore, we didn't include `Open` in our analysis.
2. `High` & `Low` & `Volume`: These three variables are numbers that can only be revealed at the end of each trading day, when the value for `Close` is also revealed. In another word, there's no way for us to know `High`, `Low` or `Volume` ahead of knowing `Close`. Therefore, it doesn't make sense to include any of these as covariates in the model to predict `Close`.
3. `Adj. Close`: The dataset we downloaded doesn't include a data dictionary about what adjustment is made to `Close`, so we weren't able to use it.

These four companies went to public at very different times, so date ranges for each company in this dataset are very different. From the boxplot, we can see that Amazon, Apple and Google's `Close` are skewed while Facebook's `Close` price is not as skewed as that of the other three companies. From line plots, we can see that before 2005, Amazon and Apple's `Close` are horizontal lines, meaning that there weren't many changes. However, starting in 2005, all four companies experienced dramatic growth in their `Close`. This observation makes sense because of the Internet's boom. To better compare all four companies' close stock price, we decided to align date ranges for them by taking a subset of observations with a date after 2012-08-05 and build models with the subset.

## Method 1: Simple Linear Models

The first method is fitting a simple linear model for each individual company. We used the data to predict the difference between the close price and the open price on each day. Since each day would have price data for all four companies, we would have to build four individual time series models. For the sake of comparing and evaluating model performances, we fit four different linear models as well, instead of using `Stock` as one of the predictors.

```{r}
#Apple
apple_lm = lm(apple_close ~ Open, data = df)
summary(apple_lm)
df$apple_naive_pred = predict(apple_lm, data=df$Open)
df$apple_naive_residual = df$apple_close - df$apple_naive_pred
ggplot(data = df, aes(x = Date)) +
  geom_line(aes(y = apple_close, color = "red")) +
  geom_line(aes(y = apple_naive_pred, color = "blue")) +
  xlab("time") +
  ylab("Apple Daily Close Stock Price") +
  ggtitle("Simple Linear Model")
rmse(df$apple_close, df$apple_naive_pred)

#Facebook
fb_lm = lm(fb_close ~ Open, data = df)
summary(fb_lm)
df$fb_naive_pred = predict(fb_lm, data=df$Open)
df$fb_naive_residual = df$fb_close - df$fb_naive_pred
ggplot(data = df, aes(x = Date)) +
  geom_line(aes(y = fb_close, color = "red")) +
  geom_line(aes(y = fb_naive_pred, color = "blue")) +
  xlab("time") +
  ylab("Facebook Daily Close Stock Price") +
  ggtitle("Simple Linear Model")
rmse(df$fb_close, df$fb_naive_pred)

#Google
google_lm = lm(google_close ~ Open, data = df)
summary(google_lm)
df$google_naive_pred = predict(google_lm, data=df$Open)
df$google_naive_residual = df$google_close - df$google_naive_pred
ggplot(data = df, aes(x = Date)) +
  geom_line(aes(y = google_close, color = "red")) +
  geom_line(aes(y = google_naive_pred, color = "blue")) +
  xlab("time") +
  ylab("Google Daily Close Stock Price") +
  ggtitle("Simple Linear Model")
rmse(df$google_close, df$google_naive_pred)
```


## Method 2: ARIMA Time Series Models

$$
\begin{aligned}
Close_t &= ARIMA_{(p, q, d) \times (P, Q, D)_s}(Close_{t-1, \cdots}) + \beta_1 * Open_{(S\&P_500)}
\end{aligned}
$$

```{r}
#Apple
apple.ts = auto.arima(df %>% select(apple_close), xreg = df$Open, seasonal = TRUE)
apple.ts %>% summary()
ggtsdisplay(apple.ts$residuals, main = "ARIMA(2,1,2)")

#Residual plot looks good and we will go with the result.
df$apple_ts_pred = c(apple.ts$fitted)
ggplot(data = df, aes(x = Date)) +
  geom_line(aes(y = apple_close, color = "red")) +
  geom_line(aes(y = apple_ts_pred, color = "blue")) +
  xlab("time") +
  ylab("Apple Daily Close Stock Price") +
  ggtitle("ARIMA(2,1,0)")
rmse(df$apple_close, df$apple_ts_pred)
```


```{r}
#Facebook
facebook.ts = auto.arima(df %>% select(fb_close), xreg = df$Open, seasonal = TRUE)
facebook.ts %>% summary()
ggtsdisplay(facebook.ts$residuals, main = "ARIMA(2,1,2)")
#After looking at the residual plot, we found spikes at period 30 and tried to see if having a seasonal AR or MA trend makes the model better.
facebook.try1 = Arima(df %>% select(fb_close), xreg = df$Open, order = c(2, 1, 2),seasonal = list(order = c(0, 0, 1),period = 30))
ggtsdisplay(facebook.try1$residuals)
facebook.try2 = Arima(df %>% select(fb_close), xreg = df$Open, order = c(2, 1, 2),seasonal = list(order = c(1, 0, 0),period = 30))
ggtsdisplay(facebook.try2$residuals)
#However, by evaluating residual plots, adding seasonal terms don't seem to give a better performance. Plus, we can see that autocorrelation with lag 30 is about 0.1, which is relatively small. So, we decided to stick with the result from auto.arima.
df$fb_ts_pred = c(facebook.ts$fitted)
ggplot(data = df, aes(x = Date)) +
  geom_line(aes(y = fb_close, color = "red")) +
  geom_line(aes(y = fb_ts_pred, color = "blue")) +
  xlab("time") +
  ylab("Facebook Daily Close Stock Price") +
  ggtitle("ARIMA(2,1,2)")
rmse(df$fb_close, df$fb_ts_pred)
```

```{r}
#Google
google.ts = auto.arima(df %>% select(google_close), xreg = df$Open, seasonal = TRUE)
google.ts %>% summary()
ggtsdisplay(google.ts$residuals, main = "ARIMA(2,1,2)")
#Although we can see spikes but given that the autocorrelation is lower than 0.04, we decided to go with the model output from auto.arima
df$google_ts_pred = c(google.ts$fitted)
ggplot(data = df, aes(x = Date)) +
  geom_line(aes(y = google_close, color = "red")) +
  geom_line(aes(y = google_ts_pred, color = "blue")) +
  xlab("time") +
  ylab("Google Daily Close Stock Price") +
  ggtitle("ARIMA(2,1,2)")
rmse(df$google_close, df$google_ts_pred)
```


## Method 3: Gaussian Process

$$
Close_t = \beta X + w_{t} \\
w_{t} \sim GP(0,\Sigma)\\
\Sigma \sim square \ exponential
$$
Because it takes a long time for JAGS to run large datasets, we subset the dataset to a year of data from 2017-4-21 to 2018-4-20.

```{r, include=FALSE}
source("util.R")
subset = df[1239:1490, ]
```

```{r}
#Facebook semivariogram
fb_emp_cloud = subset %>% emp_semivariogram(fb_naive_residual,Date)
fb_emp = rbind(
  subset %>% emp_semivariogram(fb_naive_residual, Date, bin=TRUE, binwidth=1)  %>% mutate(binwidth="binwidth=1"),
  subset %>% emp_semivariogram(fb_naive_residual, Date, bin=TRUE, binwidth=5) %>% mutate(binwidth="binwidth=5"),
  subset %>% emp_semivariogram(fb_naive_residual, Date, bin=TRUE, binwidth=10) %>% mutate(binwidth="binwidth=10"),
  subset %>% emp_semivariogram(fb_naive_residual, Date, bin=TRUE, binwidth=15)   %>% mutate(binwidth="binwidth=15"),
  subset %>% emp_semivariogram(fb_naive_residual, Date, bin=TRUE, binwidth=30)  %>% mutate(binwidth="binwidth=30")
)

fb_emp %>%
  ggplot(aes(x=h, y=gamma)) +
  geom_point(size = 1) +
  ggtitle("Empirical Semivariogram of Facebook (binned)")+
  facet_wrap(~binwidth, nrow=2)
```

```{r}
fb_gp_exp_model = "model{
  y ~ dmnorm(mu, inverse(Sigma))

  for (i in 1:N) {
    mu[i] <- beta[1]+ beta[2] * x[i]
  }
  
  for (i in 1:(N-1)) {
    for (j in (i+1):N) {
      Sigma[i,j] <- sigma2 * exp(- pow(l*d[i,j],2))
      Sigma[j,i] <- Sigma[i,j]
    }
  }

  for (k in 1:N) {
    Sigma[k,k] <- sigma2 + sigma2_w
  }

  for (i in 1:2) {
    beta[i] ~ dt(coef[i], 2.5, 1)
  }
  sigma2_w ~ dnorm(10, 1/25) T(0,)
  sigma2   ~ dnorm(390, 1/200) T(0,)
  l        ~ dt(0,2.5,1) T(0,) 
}"
```

```{r}
if (file.exists("fb_gp_jags.Rdata")) {
  load(file="fb_gp_jags.Rdata")
} else {
  m = rjags::jags.model(
    textConnection(fb_gp_exp_model), 
    data = list(
      y = subset$fb_close,
      x = subset$Open,
      d = dist(subset$Date) %>% as.matrix(),
      N = nrow(subset),
      coef = coef(fb_lm)
    ),
    quiet = TRUE
  )

  update(m, n.iter=2000)

  exp_cov_coda = rjags::coda.samples(
    m, variable.names=c("beta", "sigma2", "l", "sigma2_w"),
    n.iter=2000, thin=10
  )
  save(exp_cov_coda, file="fb_gp_jags.Rdata")
}
```

```{r}
betas = tidybayes::gather_draws(exp_cov_coda, beta[i]) %>%
  ungroup() %>%
  mutate(.variable = paste0(.variable, "[",i,"]")) %>%
  select(-i)
betas %>%
  group_by(.variable) %>%
  slice(seq(1,n(),length.out=500)) %>%
  ggplot(aes(x=.iteration, y=.value, color=.variable)) +
    geom_line() +
    facet_grid(.variable~., scales = "free_y")
params = tidybayes::gather_draws(exp_cov_coda, sigma2, l, sigma2_w)
params %>%
  slice(seq(1,n(),length.out=500)) %>%
  ggplot(aes(x=.iteration, y=.value, color=.variable)) +
    geom_line() +
    facet_grid(.variable~., scales="free_y")
params %>%
  slice(seq(1,n(),length.out=500)) %>%
  ggplot(aes(x=.value, fill=.variable)) +
    geom_density() +
    facet_wrap(~.variable, scales="free") +
    guides(fill=FALSE)
params %>%
  slice(seq(1,n(),length.out=500)) %>% 
  filter(.variable == "l") %>%
  ggplot(aes(x=.value, fill=.variable)) +
    geom_density() +
    scale_x_log10() +
    facet_wrap(~.variable, scales="free") +
    guides(fill=FALSE)
post = bind_rows(betas, params) %>%
  group_by(.variable) %>%
  summarize(
    post_mean = mean(.value),
    post_med  = median(.value),
    post_lower = quantile(.value, probs = 0.025),
    post_upper = quantile(.value, probs = 0.975)
  )
knitr::kable(post, digits = 5)
l = post %>% filter(.variable == 'l') %>% pull(post_med)
sigma2 = post %>% filter(.variable == 'sigma2') %>% pull(post_med)
sigma2_w = post %>% filter(.variable == 'sigma2_w') %>% pull(post_med)
beta0 = post %>% filter(.variable == 'beta[1]') %>% pull(post_med)
beta1 = post %>% filter(.variable == 'beta[2]') %>% pull(post_med)
df = df %>% mutate(fb_gp_resid = fb_close - beta0 - beta1 * Open)

reps=1000
x = df$Open
y = df$fb_close
x_pred = df$Open + rnorm(365, 0.01)
mu = beta0 + beta1*x
mu_pred = beta0 + beta1*x_pred
dist_o = fields::rdist(x)
dist_p = fields::rdist(x_pred)
dist_op = fields::rdist(x, x_pred)
dist_po = t(dist_op)
cov_o  = sq_exp_cov(dist_o,  sigma2 = sigma2, l = l, sigma2_w = sigma2_w)
cov_p  = sq_exp_cov(dist_p,  sigma2 = sigma2, l = l, sigma2_w = sigma2_w)
cov_op = sq_exp_cov(dist_op, sigma2 = sigma2, l = l, sigma2_w = sigma2_w)
cov_po = sq_exp_cov(dist_po, sigma2 = sigma2, l = l, sigma2_w = sigma2_w)
cond_cov = cov_p - cov_po %*% solve(cov_o) %*% cov_op
cond_mu  = mu_pred + cov_po %*% solve(cov_o) %*% (y - mu)
pred_bayes = cond_mu %*% matrix(1, ncol=reps) + t(chol(cond_cov)) %*% matrix(rnorm(length(x_pred)*reps), ncol=reps)
pred_df_bayes = pred_bayes %>% t() %>% post_summary() %>% mutate(x=x_pred)





rmse(new_df$fb_close, new_df$fb_bayes_predict)

```



## Conclusion

## Discussion
